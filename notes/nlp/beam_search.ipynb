{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Beam Search 原理及实现\n",
    "\n",
    "## 原理\n",
    "Beam search 是 seq2seq 模型在 decoder 过程中寻找次优解的一个方法。\n",
    "\n",
    "Beam search 的原理比较简单，就是维护一个 beam size 大小的当前最优（即概率最大） sequence。然后每次 decode 下一个 token 的时候，就有 beam_size * vocab_size 种组合情况，继续从这些组合种找出 beam_size 个概率最大的序列，依次类推。\n",
    "\n",
    "由于每次加一个 token，序列概率都倾向于变小（序列概率是连乘的），因此可以加一个 [length normalization](https://arxiv.org/pdf/1609.08144.pdf) 来消减这种倾向。\n",
    "\n",
    "## 实现\n",
    "在实现的时候，要用`tf.while_loop`来推进 decode 的进行。在 decode 过程中会维护两个 beam_size 大小的学列，一个用于存放已经到达 eos token 的序列，即已经完成的序列，一个用于存放还没有完成的序列。\n",
    "\n",
    "主要需要考虑以下几点：\n",
    "1. Decode 停止条件：\n",
    "    * 达到最大 decode 长度，最大 decode 长度是认为设定的，可以根据不同的 task 决定。\n",
    "    * 已经生成的序列的最低分比当前序列的最高分还高，即找不到更好预测序列了。\n",
    "2. Decode 的一个 step:\n",
    "    * 当前预测序列作为输入，得到下一个预测 token 的概率分布。\n",
    "    * 找到 beam size 个序列概率最大的序列。\n",
    "   \n",
    "下面的代码是 official 实现的 transformer 中，beam search 的实现。代码基本都是抄下来的，不过抄的过程仍然受益匪浅: )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.0.0-beta1\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "print(tf.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "INF = 1. * 1e7\n",
    "\n",
    "class _StateKeys(object):\n",
    "    \"\"\"State 中的 key 的定义\n",
    "    \"\"\"\n",
    "    #TODO: 每个加一个注释.\n",
    "    CUR_INDEX = \"CUR_INDEX\"\n",
    "    ALIVE_SEQ = \"ALIVE_SEQ\"\n",
    "    ALIVE_LOG_PROBS = \"ALIVE_LOG_PROBS\"\n",
    "    ALIVE_CACHE = \"ALIVE_CACHE\"\n",
    "    FINISHED_SEQ = \"FINISHED_SEQ\"\n",
    "    FINISHED_SCORES = \"FINISHED_SCORES\"\n",
    "    FINISHED_FLAGS = \"FINISHED_FLAGS\"\n",
    "\n",
    "\n",
    "class SequenceBeamSearch:\n",
    "    def __init__(self, symbols_to_logits_fn, vocab_size, batch_size, beam_size, alpha, max_decode_length, eos_id):\n",
    "        \"\"\"\n",
    "        \"\"\"\n",
    "        self.symbols_to_logits_fn = symbols_to_logits_fn\n",
    "        self.vocab_size = vocab_size\n",
    "        self.batch_size = batch_size\n",
    "        self.beam_size = beam_size\n",
    "        self.alpha = alpha\n",
    "        self.max_decode_length = max_decode_length\n",
    "        self.eos_id = eos_id\n",
    "        \n",
    "    def search(self, initial_ids, initial_cache):\n",
    "        state, state_shapes = self._create_initial_state(initial_ids, initial_cache)\n",
    "        \n",
    "        finished_state = tf.while_loop(\n",
    "            self._continue_search, self._search_step, loop_vars=[state],\n",
    "            shape_invariants=[state_shapes], parallel_iterations=1, back_prop=False)\n",
    "        finished_state = finished_state[0]\n",
    "\n",
    "        alive_seq = finished_state[_StateKeys.ALIVE_SEQ]\n",
    "        alive_log_probs = finished_state[_StateKeys.ALIVE_LOG_PROBS]\n",
    "        finished_seq = finished_state[_StateKeys.FINISHED_SEQ]\n",
    "        finished_scores = finished_state[_StateKeys.FINISHED_SCORES]\n",
    "        finished_flags = finished_state[_StateKeys.FINISHED_FLAGS]\n",
    "        \n",
    "        # 由于有可能 finished_seq 里一个序列都没有，即没有任何一个序列走到了 eos token，这时候需要把\n",
    "        # alive_seq 作为 backup.\n",
    "        finished_cond = tf.reduce_any(finished_flags, 1, name=\"finished_cond\")\n",
    "        seq_cond = _expand_to_same_rank(finished_cond, finished_seq)\n",
    "        score_cond = _expand_to_same_rank(finished_cond, finished_scores)\n",
    "        finished_seq = tf.where(seq_cond, finished_seq, alive_seq)\n",
    "        finished_scores = tf.where(score_cond, finished_scores, alive_log_probs)\n",
    "        return finished_seq, finished_scores\n",
    "        \n",
    "    def _create_initial_state(self, initial_ids, initial_cache):\n",
    "        \"\"\"inital_ids: 预测时的初始 id (一般设为0)，维度为 (batch_size, )\n",
    "        如果 batch_size 为 3， 则 initial_ids 为 [0, 0, 0]\n",
    "        \"\"\"\n",
    "        # 当前 decode 到哪个位置，初始为 0\n",
    "        cur_index = tf.constant(0)\n",
    "        \n",
    "        # 还没有 decode 完成的 sequence， 即没有decode 到 eos token.\n",
    "        alive_seq = _expand_to_beam_size(initial_ids, self.beam_size)\n",
    "        # (batch_size, beam_size, 1)\n",
    "        alive_seq = tf.expand_dims(alive_seq, axis=2)\n",
    "        \n",
    "        # alive_log_probs 保存每个 batch 每个 beam 下的 sequence 的 log probability。\n",
    "        # 初始化 sequence 的概率为1，即 log prob 为 0.\n",
    "        # 维度为 (batch_size, beam_size)\n",
    "        # 例如，当 batch size 为3， beam size 为4时，alive_log_probs 初始化为:\n",
    "        # [[  0. -inf -inf -inf]\n",
    "        #  [  0. -inf -inf -inf]\n",
    "        #  [  0. -inf -inf -inf]]\n",
    "        initial_log_probs = tf.constant([[0.] + [-float(\"inf\")] * (self.beam_size - 1)])\n",
    "        alive_log_probs = tf.tile(initial_log_probs, [self.batch_size, 1])\n",
    "        \n",
    "        # 将 cache 中保存的每一个变量都加一维 beam_size 维，使得不同 beam 下 cache 的变量不一样。\n",
    "        alive_cache = tf.nest.map_structure(lambda t: _expand_to_beam_size(t, self.beam_size), initial_cache)\n",
    "        \n",
    "        # 初始化用户保存已经预测完成的 sequence 的变量。\n",
    "        finished_seq = tf.zeros(tf.shape(alive_seq), tf.int32)\n",
    "        # 初始化用户保存已经预测完成的 sequence 的 log probability.\n",
    "        finished_scores = tf.ones([self.batch_size, self.beam_size]) * -INF\n",
    "        # 初始化用户保存 sequence 是否已经预测完成的变量。\n",
    "        finished_flags = tf.zeros([self.batch_size, self.beam_size], tf.bool)\n",
    "        \n",
    "        # 初始化 state，这个 state 命名是根据 tf.while_loop 来的（类比 rnn 中的初始化 state）。\n",
    "        state = {\n",
    "            _StateKeys.CUR_INDEX: cur_index,\n",
    "            _StateKeys.ALIVE_SEQ: alive_seq,\n",
    "            _StateKeys.ALIVE_LOG_PROBS: alive_log_probs,\n",
    "            _StateKeys.ALIVE_CACHE: alive_cache,\n",
    "            _StateKeys.FINISHED_SEQ: finished_seq,\n",
    "            _StateKeys.FINISHED_SCORES: finished_scores,\n",
    "            _StateKeys.FINISHED_FLAGS: finished_flags\n",
    "        }\n",
    "        \n",
    "        # 在 tf.while_loop 为了保证正确性，每个 loop 都会检查 state 中变量的 shape 是不是和 shape_invariants 设置的 shape 保持一致。\n",
    "        # 如果不一致，就会报错。因此，如果 state 中的变量在 loop 的时候 shape 会变，则需要把它设置的 general 一点，比如 None。\n",
    "        # 另外，如果 dimension 的值会根据 state 的输入不同而不同，不能提前确定，也要设置成 None，比如 batch size.\n",
    "        state_shape_invariants = {\n",
    "            _StateKeys.CUR_INDEX: tf.TensorShape([]),\n",
    "            _StateKeys.ALIVE_SEQ: tf.TensorShape([None, self.beam_size, None]),\n",
    "            _StateKeys.ALIVE_LOG_PROBS: tf.TensorShape([None, self.beam_size]),\n",
    "            _StateKeys.ALIVE_CACHE: tf.nest.map_structure(\n",
    "                _get_shape_keep_last_dim, alive_cache),\n",
    "            _StateKeys.FINISHED_SEQ: tf.TensorShape([None, self.beam_size, None]),\n",
    "            _StateKeys.FINISHED_SCORES: tf.TensorShape([None, self.beam_size]),\n",
    "            _StateKeys.FINISHED_FLAGS: tf.TensorShape([None, self.beam_size])\n",
    "        }\n",
    "        \n",
    "        return state, state_shape_invariants\n",
    "    \n",
    "    def _continue_search(self, state):\n",
    "        \"\"\"判断 decode 是否应该停止，decode 停止条件有两个：\n",
    "            1. 达到最大 decode 长度。\n",
    "            2. 已经生成的序列的最低分比当前序列的最高分还高，即找不到更好预测序列了。\n",
    "        \"\"\"\n",
    "        i = state[_StateKeys.CUR_INDEX]\n",
    "        alive_log_probs = state[_StateKeys.ALIVE_LOG_PROBS]\n",
    "        finished_scores = state[_StateKeys.FINISHED_SCORES]\n",
    "        finished_flags = state[_StateKeys.FINISHED_FLAGS]\n",
    "    \n",
    "        not_at_max_decode_length = tf.less(i, self.max_decode_length)\n",
    "        max_length_norm = _length_normalization(self.alpha, self.max_decode_length)\n",
    "        \n",
    "        # 为什么是取第0个beam，因为存的时候就是排序好的。\n",
    "        best_alive_scores = alive_log_probs[:, 0] / max_length_norm\n",
    "        \n",
    "        finished_scores *= tf.cast(finished_flags, tf.float32)\n",
    "        # 当前预测完成的序列的最低分, 维度 (batch_size, )\n",
    "        lowest_finished_scores = tf.reduce_min(finished_scores, axis=1)\n",
    "        \n",
    "        # 如果某个batch一个已完成的序列都没有，则把分数设为一个最小值。\n",
    "        finished_batches = tf.reduce_any(finished_flags, 1)\n",
    "        lowest_finished_scores += (1.0 - tf.cast(finished_batches, tf.float32)) * -INF\n",
    "        \n",
    "        worst_finished_score_better_than_best_alive_score = tf.reduce_all(\n",
    "            tf.greater(lowest_finished_scores, best_alive_scores)\n",
    "        )\n",
    "\n",
    "        return tf.logical_and(\n",
    "            not_at_max_decode_length,\n",
    "            tf.logical_not(worst_finished_score_better_than_best_alive_score)\n",
    "        )\n",
    "        \n",
    "    def _search_step(self, state):\n",
    "        # Step 1. 对于每一个 batch 的每一个 beam，都去 decode 下一个 token。并保留 beam_size * 2个概率最高的序列。\n",
    "        # 保留 beam_size * 2 的目的是保证至少有 beam_size 个序列是还没 decode 完成的。例如假如每个 beam 都是 eos token 概率\n",
    "        # 最高，多取一个可以保证能取到非 eos 的 token。\n",
    "        new_seq, new_log_probs, new_cache = self._grow_alive_seq(state)\n",
    "\n",
    "        # Step 2. 从 beam_size * 2 个概率最高的序列中，拿出 beam_size 个概率最高的，且还没有 decode 完成的序列。\n",
    "        alive_state = self._get_new_alive_state(new_seq, new_log_probs, new_cache)\n",
    "        \n",
    "        # Step 3. 把新得到的已完成的序列与原来得到的已完成的序列拼在一起，得到新的 beam_size 个 log prob 最高的「已完成」序列。\n",
    "        finished_state = self._get_new_finished_state(state, new_seq, new_log_probs)\n",
    "        \n",
    "        # Step 4. 更新 state.\n",
    "        new_state = {_StateKeys.CUR_INDEX: state[_StateKeys.CUR_INDEX] + 1}\n",
    "        new_state.update(alive_state)\n",
    "        new_state.update(finished_state)\n",
    "        return [new_state]\n",
    "        \n",
    "    def _get_new_finished_state(self, state, new_seq, new_log_probs):\n",
    "        i = state[_StateKeys.CUR_INDEX]\n",
    "        finished_seq = state[_StateKeys.FINISHED_SEQ]\n",
    "        finished_scores = state[_StateKeys.FINISHED_SCORES]\n",
    "        finished_flags = state[_StateKeys.FINISHED_FLAGS]\n",
    "        \n",
    "        length_norm = _length_normalization(self.alpha, i + 1)\n",
    "        new_scores = new_log_probs / length_norm\n",
    "        \n",
    "        new_finished_flags = tf.equal(new_seq[:, :, -1], self.eos_id)\n",
    "        new_scores += (1 - tf.cast(new_finished_flags, tf.float32)) * -INF\n",
    "        \n",
    "        finished_seq = tf.concat([finished_seq, tf.zeros([self.batch_size, self.beam_size, 1], tf.int32)], axis=2)\n",
    "        \n",
    "        finished_seq = tf.concat([finished_seq, new_seq], axis=1)\n",
    "        finished_scores = tf.concat([finished_scores, new_scores], axis=1)\n",
    "        finished_flags = tf.concat([finished_flags, new_finished_flags], axis=1)\n",
    "        \n",
    "        top_finished_seq, top_finished_scores, top_finished_flags = (\n",
    "            _gather_topk_beams([finished_seq, finished_scores, finished_flags],\n",
    "                               finished_scores, self.batch_size, self.beam_size))\n",
    "        \n",
    "        return {\n",
    "            _StateKeys.FINISHED_SEQ: top_finished_seq,\n",
    "            _StateKeys.FINISHED_SCORES: top_finished_scores,\n",
    "            _StateKeys.FINISHED_FLAGS: top_finished_flags\n",
    "        }\n",
    "        \n",
    "    def _grow_alive_seq(self, state):\n",
    "        \"\"\" 对于还没有decode完成的每一个 sequence，继续decode下一个词，并保留 beam_size * 2 个序列概率最大的序列。\n",
    "        Returns:\n",
    "          topk_seq: 概率最大的topk个序列，shape: (batch_size, beam_size*2, i+2)\n",
    "          topk_log_probs: topk个序列对应的log prob，shape: (batch_size, beam_size*2)\n",
    "          new_cache: 序列对应的 attention 中的 k, v 等信息。\n",
    "        \"\"\"\n",
    "        i = state[_StateKeys.CUR_INDEX]\n",
    "        alive_seq = state[_StateKeys.ALIVE_SEQ]\n",
    "        alive_log_probs = state[_StateKeys.ALIVE_LOG_PROBS]\n",
    "        alive_cache = state[_StateKeys.ALIVE_CACHE]\n",
    "        \n",
    "        beams_to_keep = 2 * self.beam_size\n",
    "        \n",
    "        \n",
    "        # 把 batch_size 和 beam_size 合并，以便喂到模型中。因为模型并不接受 beam_size 这一维\n",
    "        flat_ids = _flatten_beam_dim(alive_seq)\n",
    "        flat_cache = tf.nest.map_structure(_flatten_beam_dim, alive_cache)\n",
    "        flat_logits, flat_cache = self.symbols_to_logits_fn(flat_ids, i, flat_cache)\n",
    "        \n",
    "        # shape: [batch_size, beam_size, vocab_size]\n",
    "        logits = _unflatten_beam_dim(flat_logits, self.batch_size, self.beam_size)\n",
    "        new_cache = tf.nest.map_structure(lambda t: _unflatten_beam_dim(t, self.batch_size, self.beam_size), flat_cache)\n",
    "        \n",
    "        # shape: [batch_size, beam_size, vocab_size] 即下一个词为词表中每个词的 log prob.\n",
    "        candidate_log_probs = _log_prob_from_logits(logits)\n",
    "        log_probs = candidate_log_probs + tf.expand_dims(alive_log_probs, axis=2)\n",
    "        \n",
    "        # 对于每个 batch，都有 beam_size * vocab_size 个 candidate 序列，我们需要从这些序列中找出 log prob 最高的 topk 个。\n",
    "        flat_log_probs = tf.reshape(log_probs, [-1, self.beam_size * self.vocab_size])\n",
    "        topk_log_probs, topk_indices = tf.nn.top_k(flat_log_probs, k=beams_to_keep)\n",
    "        \n",
    "        # shape: (batch_size, beams_to_keep)\n",
    "        topk_beam_indices = topk_indices // self.vocab_size\n",
    "        \n",
    "        topk_seq, new_cache = _gather_beams(\n",
    "            [alive_seq, new_cache], topk_beam_indices, self.batch_size,\n",
    "            beams_to_keep)\n",
    "        \n",
    "        topk_word_ids = topk_indices % self.vocab_size\n",
    "        # shape: (batch_size, beams_to_keep, 1)\n",
    "        topk_word_ids = tf.expand_dims(topk_word_ids, axis=2)\n",
    "        topk_seq = tf.concat([topk_seq, topk_word_ids], axis=2)\n",
    "        return topk_seq, topk_log_probs, new_cache\n",
    "    \n",
    "    def _get_new_alive_state(self, new_seq, new_log_probs, new_cache):\n",
    "        new_finished_flags = tf.equal(new_seq[:, :, -1], self.eos_id)\n",
    "        new_log_probs += tf.cast(new_finished_flags, tf.float32) * -INF\n",
    "        \n",
    "        top_alive_seq, top_alive_log_probs, top_alive_cache = _gather_topk_beams(\n",
    "            [new_seq, new_log_probs, new_cache], new_log_probs, self.batch_size, self.beam_size)\n",
    "        \n",
    "        return {\n",
    "            _StateKeys.ALIVE_SEQ: top_alive_seq,\n",
    "            _StateKeys.ALIVE_LOG_PROBS: top_alive_log_probs,\n",
    "            _StateKeys.ALIVE_CACHE: top_alive_cache\n",
    "        }\n",
    "        \n",
    "def _gather_topk_beams(nested, score_or_log_prob, batch_size, beam_size):\n",
    "    _, topk_indexes = tf.nn.top_k(score_or_log_prob, k=beam_size)\n",
    "    return _gather_beams(nested, topk_indexes, batch_size, beam_size)\n",
    "\n",
    "def _expand_to_beam_size(tensor, beam_size):\n",
    "    \"\"\"给 tensor 添加一维 beam_size 的维度，添加到第一维。比如 tensor 是 (batch_size, ) 则输出是 (batch_size, beam_size) \n",
    "    例如: tensor = [1, 2, 3, 4], beam_size 是3，则结果为: \n",
    "    [\n",
    "     [1, 1, 1]\n",
    "     [2, 2, 2]\n",
    "     [3, 3, 3]\n",
    "     [4, 4, 4]\n",
    "    ]\n",
    "    \"\"\"\n",
    "    tensor = tf.expand_dims(tensor, axis=1)\n",
    "    tile_dims = [1] * tensor.shape.ndims\n",
    "    tile_dims[1] = beam_size\n",
    "    \n",
    "    return tf.tile(tensor, tile_dims)\n",
    "\n",
    "def _get_shape_keep_last_dim(tensor):\n",
    "    \"\"\"只保留 shape 的最后一维，其它都设为 None。\n",
    "    \"\"\"\n",
    "    shape_list = _shape_list(tensor)\n",
    "    \n",
    "    for i in range(len(shape_list) - 1):\n",
    "        shape_list[i] = None\n",
    "    \n",
    "    # 这句话用在什么情况？\n",
    "    if isinstance(shape_list[-1], tf.Tensor):\n",
    "        shape_list[-1] = None\n",
    "    \n",
    "    return tf.TensorShape(shape_list)\n",
    "    \n",
    "def _shape_list(tensor):\n",
    "    shape = tensor.get_shape().as_list()\n",
    "    dynamic_shape = tf.shape(tensor)\n",
    "    for i in range(len(shape)):\n",
    "        if shape[i] is None:\n",
    "            shape[i] = dynamic_shape[i]\n",
    "    return shape\n",
    "    \n",
    "def _length_normalization(alpha, length):\n",
    "    \"\"\"长度归一化，使得 beam search 给短的 sequence 一些惩罚。\n",
    "    \"\"\"\n",
    "    return tf.pow(((5. + tf.cast(length, tf.float32)) / 6.), alpha)\n",
    "\n",
    "def _flatten_beam_dim(tensor):\n",
    "    \"\"\" 合并 batch_size 和 beam_size 这俩维到 batch_size * beam_size 一维。\n",
    "    即 (batch_size, beam_size, ...) -> (batch_size * beam_size, ...)\n",
    "    \"\"\"\n",
    "    shape = _shape_list(tensor)\n",
    "    shape[0] *= shape[1]\n",
    "    shape.pop(1)\n",
    "    return tf.reshape(tensor, shape)\n",
    "\n",
    "def _unflatten_beam_dim(tensor, batch_size, beam_size):\n",
    "    \"\"\" 与 flatten_beam_dim 效果相反。\n",
    "    即：(batch_size * beam_size, ...) -> (batch_size, beam_size, ...)\n",
    "    \"\"\"\n",
    "    shape = _shape_list(tensor)\n",
    "    new_shape = [batch_size, beam_size] + shape[1:]\n",
    "    return tf.reshape(tensor, new_shape)\n",
    "\n",
    "def _log_prob_from_logits(logits):\n",
    "    \"\"\" 计算log概率： log(exp(xi) / sigma(exp(xj)))\n",
    "    \"\"\"\n",
    "    return logits - tf.reduce_logsumexp(logits, axis=2, keepdims=True)\n",
    "\n",
    "def _gather_beams(nested, beam_indices, batch_size, new_beam_size):\n",
    "    # 生成一个 batch_size * new_beam_size 的 tensor，每个 batch 下面都是对应的 batch 下标。\n",
    "    # 例如 batch_size = 2, new_beam_size = 3, 则 batch_pos 为:\n",
    "    # [[0, 0, 0],\n",
    "    #  [1, ,1 ,1]]\n",
    "    batch_pos = tf.range(batch_size * new_beam_size) // new_beam_size\n",
    "    batch_pos = tf.reshape(batch_pos, [batch_size, new_beam_size])\n",
    "    \n",
    "    # 把 batch_pos 和 beam_indices 拼在一起，得到一个 (batch_size, new_beam_size, 2) 的指示下标。\n",
    "    # 最后一维的每个元素都是一个 (batch下标, beam下标).\n",
    "    # 这个是用于传给 tf.gather_nd 来获取对应下标的元素的。\n",
    "    indices = tf.stack([batch_pos, beam_indices], axis=2)\n",
    "    \n",
    "    return tf.nest.map_structure(lambda state: tf.gather_nd(state, indices), nested)\n",
    "\n",
    "def _expand_to_same_rank(tensor, target):\n",
    "  if tensor.shape.rank is None:\n",
    "    raise ValueError(\"Expect rank for tensor shape, but got None.\")\n",
    "  if target.shape.rank is None:\n",
    "    raise ValueError(\"Expect rank for target shape, but got None.\")\n",
    "\n",
    "  with tf.name_scope(\"expand_rank\"):\n",
    "    diff_rank = target.shape.rank - tensor.shape.rank\n",
    "    for _ in range(diff_rank):\n",
    "      tensor = tf.expand_dims(tensor, -1)\n",
    "    return tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "seq: (2, 3, 11)\n",
      "seq score: (2, 3)\n"
     ]
    }
   ],
   "source": [
    "batch_size = 2\n",
    "beam_size = 3\n",
    "vocab_size = 4\n",
    "max_decode_length = 10\n",
    "\n",
    "def symbols_to_logits_fn(ids, i, cache):\n",
    "    logits = tf.ones([batch_size * beam_size, vocab_size])\n",
    "    return logits, cache\n",
    "    \n",
    "def test_beam_search():\n",
    "    initial_ids = tf.zeros([batch_size], dtype=tf.int32)\n",
    "    initial_cache = {}\n",
    "    searcher = SequenceBeamSearch(symbols_to_logits_fn, vocab_size, batch_size, beam_size, 0.6, max_decode_length, 99)\n",
    "    seq, seq_score = searcher.search(initial_ids, initial_cache)\n",
    "    print(f'seq: {seq.shape}')\n",
    "    print(f'seq score: {seq_score.shape}')\n",
    "    \n",
    "test_beam_search()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "可以看到，beam search 最终的输出结果有两个，分别是 batch size * beam size 个预测序列以及其对应的序列概率（其实是 log probability）。\n",
    "\n",
    "序列长度可能不一样，因此0表示 padding。"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:py36]",
   "language": "python",
   "name": "conda-env-py36-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
