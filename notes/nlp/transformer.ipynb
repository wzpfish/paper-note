{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.0.0-beta1\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import math\n",
    "print(tf.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 一 基础知识\n",
    "## 如何实现 tf.keras.layers.Layer\n",
    "\n",
    "根据 [tf.keras.layers.Layer 文档](https://www.tensorflow.org/api_docs/python/tf/keras/layers/Layer#class_layer)，实现一个自定义的 layer 需要实现三个方法:\n",
    "1. \\_\\_init\\_\\_(): 保存一些设置\n",
    "2. build(input_shape): 通常用于创建 layer 需要的参数。\n",
    "3. call(inputs, **kwargs): 前向的逻辑。\n",
    "\n",
    "例如下面自定义的 Dense Layer:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dense: (5, 10) x (10, 20) -> (5, 20)\n"
     ]
    }
   ],
   "source": [
    "class MyDense(tf.keras.layers.Layer):\n",
    "    def __init__(self, hidden_size):\n",
    "        super(MyDense, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "    \n",
    "    def build(self, input_shape):\n",
    "        length = input_shape.as_list()[-1]\n",
    "        with tf.name_scope('weights'):\n",
    "            self.dense_weights = self.add_weight(\n",
    "                'weights',\n",
    "                shape=[length, self.hidden_size],\n",
    "                dtype='float32',\n",
    "                initializer=tf.random_normal_initializer(mean=0, stddev=0.1))\n",
    "        super(MyDense, self).build(input_shape)\n",
    "    \n",
    "    def call(self, inputs):\n",
    "        with tf.name_scope('dense'):\n",
    "            outputs = tf.matmul(inputs, self.dense_weights)\n",
    "            return outputs\n",
    "        \n",
    "    def get_config(self):\n",
    "        return {\n",
    "            'hidden_size': self.hidden_size\n",
    "        }\n",
    "    \n",
    "def test_dense():\n",
    "    layer = MyDense(20)\n",
    "    x = tf.ones([5, 10])\n",
    "    y = layer(x)\n",
    "    print(f\"Dense: (5, 10) x (10, 20) -> {y.shape}\")\n",
    "    \n",
    "test_dense()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 如何实现 tf.keras.Model\n",
    "\n",
    "根据 [tf.keras.Model 文档](https://www.tensorflow.org/api_docs/python/tf/keras/Model)，实现一个自定义的 model 需要实现两个方法：\n",
    "1. \\_\\_init\\_\\_(): 保存一些设置，通常设置 model 的 layer 信息。\n",
    "2. call(inputs, training): 模型的前向计算。\n",
    "\n",
    "例如下面的例子："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(8, 16) x model -> (8, 5)\n"
     ]
    }
   ],
   "source": [
    "class MyModel(tf.keras.Model):\n",
    "  def __init__(self):\n",
    "    super(MyModel, self).__init__()\n",
    "    self.dense1 = tf.keras.layers.Dense(4, activation=tf.nn.relu)\n",
    "    self.dense2 = tf.keras.layers.Dense(5, activation=tf.nn.softmax)\n",
    "    self.dropout = tf.keras.layers.Dropout(0.5)\n",
    "\n",
    "  def call(self, inputs, training=False):\n",
    "    x = self.dense1(inputs)\n",
    "    if training:\n",
    "      x = self.dropout(x, training=training)\n",
    "    return self.dense2(x)\n",
    "\n",
    "def test_model():\n",
    "    model = MyModel()\n",
    "    x = tf.ones([8, 16])\n",
    "    y = model.predict(x)\n",
    "    print(f'{x.shape} x model -> {y.shape}' )\n",
    "test_model()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 二 实现 transformer\n",
    "## Embedding 层\n",
    "根据论文的第3.4小节，在 transformer 中，emebedding 层和 输出的 softmax 之前的映射层是共享权重的。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Embedding: (8, 32) x (100, 256) -> (8, 32, 256)\n",
      "Linear: (8, 32, 256) x (100, 256) -> (8, 32, 100)\n"
     ]
    }
   ],
   "source": [
    "class EmbeddingSharedWeights(tf.keras.layers.Layer):\n",
    "    \"\"\"实现论文的3.4节.\n",
    "    \"\"\"\n",
    "    def __init__(self, vocab_size, hidden_size):\n",
    "        super(EmbeddingSharedWeights, self).__init__()\n",
    "        self.vocab_size = vocab_size\n",
    "        self.hidden_size = hidden_size\n",
    "        \n",
    "    def build(self, input_shape):\n",
    "        # 这里权重初始化方法是随便选的..\n",
    "        self.shared_weights = self.add_weight(\n",
    "            \"weights\",\n",
    "            shape=[self.vocab_size, self.hidden_size],\n",
    "            dtype='float32',\n",
    "            initializer=tf.random_normal_initializer(\n",
    "              mean=0., stddev=self.hidden_size**-0.5))\n",
    "        super(EmbeddingSharedWeights, self).build(input_shape)\n",
    "        \n",
    "    def call(self, inputs, mode='embedding'):\n",
    "        if mode == 'embedding':\n",
    "            return self._embedding(inputs)\n",
    "        elif mode == 'linear':\n",
    "            return self._linear(inputs)\n",
    "        else:\n",
    "            raise ValueError(f'unknown mode {mode}')\n",
    "        \n",
    "    def get_config(self):\n",
    "        return {\n",
    "            'vocab_size': self.vocab_size,\n",
    "            'hidden_size': self.hidden_size\n",
    "        }\n",
    "    \n",
    "    def _embedding(self, inputs):\n",
    "        \"\"\"inputs: [batch_size, length]\n",
    "        \"\"\"\n",
    "        with tf.name_scope('embedding'):\n",
    "            mask = tf.cast(tf.not_equal(inputs, 0), tf.float32)\n",
    "            # mask shape: [batch_size, length, 1]\n",
    "            mask = tf.expand_dims(mask, -1)\n",
    "            # embeddings shape: [batch_size, length, hidden_size]\n",
    "            embeddings = tf.gather(self.shared_weights, inputs)\n",
    "            embeddings *= mask\n",
    "            embeddings *= (self.hidden_size ** 0.5)\n",
    "            \n",
    "            return embeddings\n",
    "        \n",
    "    def _linear(self, inputs):\n",
    "        \"\"\"inputs: [batch_size, length, hidden_size]\n",
    "        \"\"\"\n",
    "        with tf.name_scope('presoftmax_linear'):\n",
    "            logits = tf.matmul(inputs, self.shared_weights, transpose_b=True)\n",
    "            return logits\n",
    "        \n",
    "def test_embedding_layer():\n",
    "    vocab_size = 100\n",
    "    hidden_size = 256\n",
    "    layer = EmbeddingSharedWeights(vocab_size, hidden_size)\n",
    "    \n",
    "    batch_size = 8\n",
    "    length = 32\n",
    "    emb_inputs = tf.ones([batch_size, length], tf.int32)\n",
    "    emb_outputs = layer(emb_inputs, mode='embedding')\n",
    "    print(f'Embedding: {emb_inputs.shape} x {layer.shared_weights.shape} -> {emb_outputs.shape}')\n",
    "    \n",
    "    linear_inputs = tf.ones([batch_size, length, hidden_size])\n",
    "    linear_outputs = layer(linear_inputs, mode='linear')\n",
    "    print(f'Linear: {linear_inputs.shape} x {layer.shared_weights.shape} -> {linear_outputs.shape}')\n",
    "    \n",
    "test_embedding_layer()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## position encoding\n",
    "由于全连接无法获取词的位置信息，我们需要对词的位置进行编码。编码有两种方式，要么学一个位置的 emebdding ，要么直接定义一个位置函数对位置做 encoding。Transformer 中采用的是第二种方式，参考论文3.5节。\n",
    "\n",
    "Position encoding 实现如下："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Position encoding shape: (32, 256)\n",
      "embedding+position encoding shape: (8, 32, 256)\n"
     ]
    }
   ],
   "source": [
    "def get_position_encoding(\n",
    "    length, hidden_size, min_timescale=1.0, max_timescale=1.0e4):\n",
    "  \"\"\"其实这儿 min_timescale 和 max_timescale 的含义我并没有很理解。。\n",
    "  \"\"\"\n",
    "  position = tf.cast(tf.range(length), tf.float32)\n",
    "  num_timescales = hidden_size // 2\n",
    "  log_timescale_increment = (\n",
    "      math.log(float(max_timescale) / float(min_timescale)) /\n",
    "      (tf.cast(num_timescales, tf.float32) - 1))\n",
    "  inv_timescales = min_timescale * tf.exp(\n",
    "      tf.cast(tf.range(num_timescales), tf.float32) * -log_timescale_increment)\n",
    "  scaled_time = tf.expand_dims(position, 1) * tf.expand_dims(inv_timescales, 0)\n",
    "  # 这一步与论文中不完全一致，论文中是区分奇数偶数位置的，这里是 encoding 的前一半用 sin，后一半用 cos\n",
    "  signal = tf.concat([tf.sin(scaled_time), tf.cos(scaled_time)], axis=1)\n",
    "  return signal\n",
    "\n",
    "def test_position_encoding():\n",
    "    length = 32\n",
    "    hidden_size = 256\n",
    "    encoding = get_position_encoding(length, hidden_size)\n",
    "    print(f'Position encoding shape: {encoding.shape}')\n",
    "    \n",
    "    emb = tf.ones([8, length, hidden_size])\n",
    "    emb = emb + encoding\n",
    "    print(f'embedding+position encoding shape: {emb.shape}')\n",
    "test_position_encoding()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  Attention layer\n",
    "attention 机制可以总结为把 query, key, value 映射到 output 的通用框架。其中，output 是所有 value 的加权平均，而权重由 query 和 对应的 key 计算得到。通常情况下， key 和 value 是同一个向量。在 transformer 中，这种框架用在了三个地方（论文第3.2.3节）。\n",
    "1. encoder multi-head self-attention: 在 encoder 中，query, key, value 是一样的，都是上一层的输出。例如上一层输出了 length 个维度为 hidden_size 的向量，则 self-attention 通过计算这些向量的点积和 sofmax 得到 length * length 个向量权重，最后取加权平均。Multihead 就是把向量映射到不同的空间，计算多次权重。最后把不同权重得到的加权平均拼在一起。\n",
    "2. decoder multi-head self-attention: 在 docoder 中，self-attention 大体上和 encoder 的一样。不同的一点是，由于在预测时，当前位置的词是不知道后面位置的词是什么的，为了防止在训练时当前位置和后面的位置做 attention，我们对每个位置都做了 mask，即 mask 掉当前位置的所有后面的词。\n",
    "3. encoder-decoder multi-head attention: 在 encoder-decoder attention 中，query 是 decoder 的输入（或者说是上一层的输出），key 和 value 是 encoder 的输出。这样的结构保证 decoder 的每个位置都可以和 input 的所有位置 attend 到。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Encoder attention: (2, 16, 512) -> (2, 16, 512)\n"
     ]
    }
   ],
   "source": [
    "def _float32_softmax(logits, name=None):\n",
    "  \"\"\"Computes a softmax activation in float32.\n",
    "\n",
    "  When training a model using float16, softmax is still done in float32 for\n",
    "  numeric stability.\n",
    "\n",
    "  Args:\n",
    "    logits: A tensor, with any shape accepted by `tf.nn.softmax`.\n",
    "\n",
    "  Returns:\n",
    "    A tensor with the same dtype as `logits`.\n",
    "  \"\"\"\n",
    "  input_dtype = logits.dtype\n",
    "  logits = tf.cast(logits, tf.float32)\n",
    "  output = tf.nn.softmax(logits, name=name)\n",
    "  return tf.cast(output, input_dtype)\n",
    "\n",
    "\n",
    "class Attention(tf.keras.layers.Layer):\n",
    "  \"\"\"Multi-headed attention layer.\"\"\"\n",
    "\n",
    "  def __init__(self, hidden_size, num_heads, attention_dropout):\n",
    "    \"\"\"Initialize Attention.\n",
    "\n",
    "    Args:\n",
    "      hidden_size: int, output dim of hidden layer.\n",
    "      num_heads: int, number of heads to repeat the same attention structure.\n",
    "      attention_dropout: float, dropout rate inside attention for training.\n",
    "    \"\"\"\n",
    "    if hidden_size % num_heads:\n",
    "      raise ValueError(\n",
    "          \"Hidden size ({}) must be divisible by the number of heads ({}).\"\n",
    "          .format(hidden_size, num_heads))\n",
    "\n",
    "    super(Attention, self).__init__()\n",
    "    self.hidden_size = hidden_size\n",
    "    self.num_heads = num_heads\n",
    "    self.attention_dropout = attention_dropout\n",
    "\n",
    "  def build(self, input_shape):\n",
    "    \"\"\"Builds the layer.\"\"\"\n",
    "    # Layers for linearly projecting the queries, keys, and values.\n",
    "    self.q_dense_layer = tf.keras.layers.Dense(\n",
    "        self.hidden_size, use_bias=False, name=\"q\")\n",
    "    self.k_dense_layer = tf.keras.layers.Dense(\n",
    "        self.hidden_size, use_bias=False, name=\"k\")\n",
    "    self.v_dense_layer = tf.keras.layers.Dense(\n",
    "        self.hidden_size, use_bias=False, name=\"v\")\n",
    "    self.output_dense_layer = tf.keras.layers.Dense(\n",
    "        self.hidden_size, use_bias=False, name=\"output_transform\")\n",
    "    super(Attention, self).build(input_shape)\n",
    "\n",
    "  def get_config(self):\n",
    "    return {\n",
    "        \"hidden_size\": self.hidden_size,\n",
    "        \"num_heads\": self.num_heads,\n",
    "        \"attention_dropout\": self.attention_dropout,\n",
    "    }\n",
    "\n",
    "  def split_heads(self, x):\n",
    "    \"\"\"Split x into different heads, and transpose the resulting value.\n",
    "\n",
    "    The tensor is transposed to insure the inner dimensions hold the correct\n",
    "    values during the matrix multiplication.\n",
    "\n",
    "    Args:\n",
    "      x: A tensor with shape [batch_size, length, hidden_size]\n",
    "\n",
    "    Returns:\n",
    "      A tensor with shape [batch_size, num_heads, length, hidden_size/num_heads]\n",
    "    \"\"\"\n",
    "    with tf.name_scope(\"split_heads\"):\n",
    "      batch_size = tf.shape(x)[0]\n",
    "      length = tf.shape(x)[1]\n",
    "\n",
    "      # Calculate depth of last dimension after it has been split.\n",
    "      depth = (self.hidden_size // self.num_heads)\n",
    "\n",
    "      # Split the last dimension\n",
    "      x = tf.reshape(x, [batch_size, length, self.num_heads, depth])\n",
    "\n",
    "      # Transpose the result\n",
    "      return tf.transpose(x, [0, 2, 1, 3])\n",
    "\n",
    "  def combine_heads(self, x):\n",
    "    \"\"\"Combine tensor that has been split.\n",
    "\n",
    "    Args:\n",
    "      x: A tensor [batch_size, num_heads, length, hidden_size/num_heads]\n",
    "\n",
    "    Returns:\n",
    "      A tensor with shape [batch_size, length, hidden_size]\n",
    "    \"\"\"\n",
    "    with tf.name_scope(\"combine_heads\"):\n",
    "      batch_size = tf.shape(x)[0]\n",
    "      length = tf.shape(x)[2]\n",
    "      x = tf.transpose(x, [0, 2, 1, 3])  # --> [batch, length, num_heads, depth]\n",
    "      return tf.reshape(x, [batch_size, length, self.hidden_size])\n",
    "\n",
    "  def call(self, x, y, bias, training, cache=None):\n",
    "    \"\"\"Apply attention mechanism to x and y.\n",
    "\n",
    "    Args:\n",
    "      x: a tensor with shape [batch_size, length_x, hidden_size]\n",
    "      y: a tensor with shape [batch_size, length_y, hidden_size]\n",
    "      bias: attention bias that will be added to the result of the dot product.\n",
    "      training: boolean, whether in training mode or not.\n",
    "      cache: (Used during prediction) dictionary with tensors containing results\n",
    "        of previous attentions. The dictionary must have the items:\n",
    "            {\"k\": tensor with shape [batch_size, i, key_channels],\n",
    "             \"v\": tensor with shape [batch_size, i, value_channels]}\n",
    "        where i is the current decoded length.\n",
    "\n",
    "    Returns:\n",
    "      Attention layer output with shape [batch_size, length_x, hidden_size]\n",
    "    \"\"\"\n",
    "    # Linearly project the query (q), key (k) and value (v) using different\n",
    "    # learned projections. This is in preparation of splitting them into\n",
    "    # multiple heads. Multi-head attention uses multiple queries, keys, and\n",
    "    # values rather than regular attention (which uses a single q, k, v).\n",
    "    q = self.q_dense_layer(x)\n",
    "    k = self.k_dense_layer(y)\n",
    "    v = self.v_dense_layer(y)\n",
    "\n",
    "    if cache is not None:\n",
    "      # Combine cached keys and values with new keys and values.\n",
    "      k = tf.concat([tf.cast(cache[\"k\"], k.dtype), k], axis=1)\n",
    "      v = tf.concat([tf.cast(cache[\"v\"], k.dtype), v], axis=1)\n",
    "\n",
    "      # Update cache\n",
    "      cache[\"k\"] = k\n",
    "      cache[\"v\"] = v\n",
    "\n",
    "    # Split q, k, v into heads.\n",
    "    q = self.split_heads(q)\n",
    "    k = self.split_heads(k)\n",
    "    v = self.split_heads(v)\n",
    "\n",
    "    # Scale q to prevent the dot product between q and k from growing too large.\n",
    "    depth = (self.hidden_size // self.num_heads)\n",
    "    q *= depth ** -0.5\n",
    "\n",
    "    # Calculate dot product attention\n",
    "    logits = tf.matmul(q, k, transpose_b=True)\n",
    "    logits += bias\n",
    "    weights = _float32_softmax(logits, name=\"attention_weights\")\n",
    "    if training:\n",
    "      weights = tf.nn.dropout(weights, rate=self.attention_dropout)\n",
    "    attention_output = tf.matmul(weights, v)\n",
    "\n",
    "    # Recombine heads --> [batch_size, length, hidden_size]\n",
    "    attention_output = self.combine_heads(attention_output)\n",
    "\n",
    "    # Run the combined outputs through another linear projection layer.\n",
    "    attention_output = self.output_dense_layer(attention_output)\n",
    "    return attention_output\n",
    "\n",
    "\n",
    "class SelfAttention(Attention):\n",
    "  \"\"\"Multiheaded self-attention layer.\"\"\"\n",
    "\n",
    "  def call(self, x, bias, training, cache=None):\n",
    "    return super(SelfAttention, self).call(x, x, bias, training, cache)\n",
    "\n",
    "\n",
    "def test_attention():\n",
    "    batch_size = 2\n",
    "    length = 16\n",
    "    hidden_size = 512\n",
    "    num_heads = 8\n",
    "    \n",
    "    x = tf.ones([batch_size, length, hidden_size])\n",
    "    bias = tf.zeros([batch_size, 1, 1, length])\n",
    "    layer = SelfAttention(hidden_size, num_heads, 0.5)\n",
    "    outputs = layer(x, bias, True)\n",
    "    print(f\"Encoder attention: {x.shape} -> {outputs.shape}\")\n",
    "\n",
    "test_attention()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Position-wise Feed-Forward Networks\n",
    "在 attention layer 之后，transfomer 会接两个全连接层（论文第3.3节），即:\n",
    "\n",
    "$l1 = RELU(xW1 + b1) \\ \\ \\ l2 = l1W2 + b2$\n",
    "\n",
    "需要注意的是，这里的全连接不是把上一层的输出拼在一起计算，而是每个位置单独计算，且权重是共享的。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FeedForwardNetwork: (8, 16, 256) x layer1 (256, 128) x layer2 (128, 256) -> (8, 16, 256)\n"
     ]
    }
   ],
   "source": [
    "class FeedForwardNetwork(tf.keras.layers.Layer):\n",
    "    def __init__(self, hidden_size, filter_size, relu_dropout):\n",
    "        super(FeedForwardNetwork, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.filter_size = filter_size\n",
    "        self.relu_dropout = relu_dropout\n",
    "    \n",
    "    def build(self, input_shape):\n",
    "        self.filter_dense_layer = tf.keras.layers.Dense(\n",
    "            self.filter_size,\n",
    "            use_bias=True,\n",
    "            activation=tf.nn.relu,\n",
    "            name='filter_layer')\n",
    "        self.output_dense_layer = tf.keras.layers.Dense(\n",
    "            self.hidden_size,\n",
    "            use_bias=True,\n",
    "            name='output_layer')\n",
    "        super(FeedForwardNetwork, self).build(input_shape)\n",
    "        \n",
    "    def get_config(self):\n",
    "        return {\n",
    "            'hidden_size': self.hidden_size,\n",
    "            'filter_size': self.filter_size,\n",
    "            'relu_dropout': self.relu_dropout\n",
    "        }\n",
    "    \n",
    "    def call(self, inputs, training):\n",
    "        \"\"\"inputs: [batch_size, length, hidden_size]\n",
    "        \"\"\"\n",
    "        output = self.filter_dense_layer(inputs)\n",
    "        if training:\n",
    "            output = tf.nn.dropout(output, rate=self.relu_dropout)\n",
    "        output = self.output_dense_layer(output)\n",
    "        \n",
    "        return output\n",
    "    \n",
    "def test_ffn():\n",
    "    batch_size = 8\n",
    "    length = 16\n",
    "    hidden_size = 256\n",
    "    \n",
    "    layer = FeedForwardNetwork(hidden_size, hidden_size/2, 0.5)\n",
    "    inputs = tf.zeros([batch_size, length, hidden_size])\n",
    "    outputs = layer(inputs, True)\n",
    "    layer1_shape = layer.filter_dense_layer.weights[0].shape\n",
    "    layer2_shape = layer.output_dense_layer.weights[0].shape\n",
    "    print(f'FeedForwardNetwork: {inputs.shape} x layer1 {layer1_shape} x layer2 {layer2_shape} -> {outputs.shape}')\n",
    "    \n",
    "test_ffn()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Layer Normalization\n",
    "关于 layer normalization，可以看我的这篇文章: [Layer Normalization](https://nbviewer.jupyter.org/github/wzpfish/paper-note/blob/master/notes/nlp/layer_normalization.ipynb)\n",
    "\n",
    "在这里实现如下："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(8, 16, 256), layer norm scale: (256,), layer norm bias: (256,) -> (8, 16, 256)\n"
     ]
    }
   ],
   "source": [
    "class LayerNormalization(tf.keras.layers.Layer):\n",
    "    def __init__(self, hidden_size):\n",
    "        super(LayerNormalization, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "    \n",
    "    def build(self, input_shape):\n",
    "        self.scale = self.add_weight(\n",
    "            'layer_norm_scale',\n",
    "            shape=[self.hidden_size],b\n",
    "            dtype='float32',\n",
    "            initializer=tf.ones_initializer())\n",
    "        self.bias = self.add_weight(\n",
    "            'layer_norm_bias',\n",
    "            shape=[self.hidden_size],\n",
    "            dtype='float32',\n",
    "            initializer=tf.zeros_initializer())\n",
    "        super(LayerNormalization, self).build(input_shape)\n",
    "    \n",
    "    def get_config(self):\n",
    "        return {\n",
    "            'hidden_size': self.hidden_size\n",
    "        }\n",
    "    \n",
    "    def call(self, inputs, epsilon=1e-6):\n",
    "        \"\"\"inputs: [batch_size, length, hidden_size]\n",
    "        \"\"\"\n",
    "        mean = tf.reduce_mean(inputs, axis=[-1], keepdims=True)\n",
    "        variance = tf.reduce_mean(tf.square(inputs - mean), axis=[-1], keepdims=True)\n",
    "        norm_inputs = (inputs - mean) * tf.math.rsqrt(variance + epsilon)\n",
    "        outputs = norm_inputs * self.scale + self.bias\n",
    "        return outputs\n",
    "    \n",
    "\n",
    "def test_layer_norm():\n",
    "    batch_size = 8\n",
    "    length = 16\n",
    "    hidden_size = 256\n",
    "    \n",
    "    layer = LayerNormalization(hidden_size)\n",
    "    inputs = tf.zeros([batch_size, length, hidden_size])\n",
    "    outputs = layer(inputs)\n",
    "    scale_shape = layer.scale.shape\n",
    "    bias_shape = layer.bias.shape\n",
    "    print(f'{inputs.shape}, layer norm scale: {scale_shape}, layer norm bias: {bias_shape} -> {outputs.shape}')\n",
    "\n",
    "test_layer_norm()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Encoder\n",
    "在 transformer 中，encoder 由6个相同的 layer stack 在一起，每个 layer 又由两个 sublayer stack 在一起。其中，第一个 sublayer 是 multi-head self-attention，第二个 sublayer 是 point-wise feed forward netword。在每个 sublayer 又有个 residual connection 以及 layer normalization。\n",
    "\n",
    "encoder 的实现如下："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(4, 6, 512) x encoder -> (4, 6, 512)\n"
     ]
    }
   ],
   "source": [
    "class LayerNormResidualWrapper(tf.keras.layers.Layer):\n",
    "    def __init__(self, layer, params):\n",
    "        super(LayerNormResidualWrapper, self).__init__()\n",
    "        self.layer = layer\n",
    "        self.params = params\n",
    "        \n",
    "    def build(self, input_shape):\n",
    "        self.layer_norm = LayerNormalization(self.params['hidden_size'])\n",
    "        super(LayerNormResidualWrapper, self).build(input_shape)\n",
    "        \n",
    "    def get_config(self):\n",
    "        return {\n",
    "            'params': self.params\n",
    "        }\n",
    "    \n",
    "    def call(self, inputs, *args, **kwargs):\n",
    "        training = kwargs['training']\n",
    "        outputs = self.layer_norm(inputs)\n",
    "        outputs = self.layer(outputs, *args, **kwargs)\n",
    "        \n",
    "        if training:\n",
    "            outputs = tf.nn.dropout(outputs, rate=self.params['layer_normresidual_dropout'])\n",
    "        outputs = outputs + inputs\n",
    "        return outputs\n",
    "    \n",
    "\n",
    "class EncoderStack(tf.keras.layers.Layer):\n",
    "    def __init__(self, params):\n",
    "        super(EncoderStack, self).__init__()\n",
    "        self.params = params\n",
    "        self.layers = []\n",
    "        \n",
    "    def build(self, input_shape):\n",
    "        params = self.params\n",
    "        for _ in range(params['num_hidden_layers']):\n",
    "            self_attention_layer = SelfAttention(params['hidden_size'], params['num_heads'], params['attention_dropout'])\n",
    "            feed_forward_network = FeedForwardNetwork(params['hidden_size'], params['filter_size'], params['relu_dropout'])\n",
    "        \n",
    "            self.layers.append([\n",
    "                LayerNormResidualWrapper(self_attention_layer, params),\n",
    "                LayerNormResidualWrapper(feed_forward_network, params)\n",
    "            ])\n",
    "        \n",
    "        self.output_normalization = LayerNormalization(params['hidden_size'])\n",
    "        super(EncoderStack, self).build(input_shape)\n",
    "    \n",
    "    def get_config(self):\n",
    "        return {\n",
    "            'params': params\n",
    "        }\n",
    "    \n",
    "    def call(self, inputs, attention_bias, training):\n",
    "        \"\"\"inputs: [batch_size, length, hidden_size]\n",
    "           attention_bias: [batch_size, 1, 1, length]\n",
    "        \"\"\"\n",
    "        for i, layer in enumerate(self.layers):\n",
    "            self_attention_layer = layer[0]\n",
    "            feed_forward_network = layer[1]\n",
    "            \n",
    "            with tf.name_scope('layer_{}'.format(i)):\n",
    "                with tf.name_scope('self_attention'):\n",
    "                    inputs = self_attention_layer(inputs, attention_bias, training=training)\n",
    "                with tf.name_scope('ffn'):\n",
    "                    inputs = feed_forward_network(inputs, training=training)\n",
    "        \n",
    "        outputs = self.output_normalization(inputs)\n",
    "        return outputs\n",
    "    \n",
    "\n",
    "def test_encoder_stack():\n",
    "    params = {\n",
    "        'num_hidden_layers': 6,\n",
    "        'hidden_size': 512,\n",
    "        'num_heads': 8,\n",
    "        'attention_dropout': 0.5,\n",
    "        'relu_dropout': 0.5,\n",
    "        'layer_normresidual_dropout': 0.5,\n",
    "        'filter_size': 256\n",
    "    }\n",
    "    batch_size = 4\n",
    "    length = 6\n",
    "    \n",
    "    encoder = EncoderStack(params)\n",
    "    inputs = tf.ones([batch_size, length, params['hidden_size']])\n",
    "    attention_bias = tf.zeros([batch_size, 1, 1, length])\n",
    "    outputs = encoder(inputs, attention_bias, True)\n",
    "    print(f'{inputs.shape} x encoder -> {outputs.shape}')\n",
    "    \n",
    "test_encoder_stack()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Decoder\n",
    "decoder 也是由6个相同的 layer stack 在一起。与 encoder 不同的是，每个 layer 由3个 sublayer 构成，分别是 masked multi-head self-attention, encoder-decoder attention 和 point-wise feed forward network。同样，每个 sublayer 又有个 residual connection 以及 layer normalization。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "decoder inputs: (4, 20, 512), encoder outputs: (4, 16, 512) x decoder -> (4, 20, 512)\n"
     ]
    }
   ],
   "source": [
    "def get_decoder_self_attetion_bias(length):\n",
    "    \"\"\" 如果长度为3，则每个位置 mask 掉后面的位置后得到：\n",
    "                            1 0 0\n",
    "                            1 1 0\n",
    "                            1 1 1\n",
    "        然后 reshape 成 [1, 1, length, length].\n",
    "    \"\"\"\n",
    "    neg_inf = -1e9\n",
    "    with tf.name_scope('decoder_self_attention_bias'):\n",
    "        # 计算下三角矩阵.\n",
    "        valid_locs = tf.linalg.band_part(tf.ones([length, length]), -1, 0)\n",
    "        # 为了适配 attention 计算时的 [batch_size, num_head, length, length]\n",
    "        valid_locs = tf.reshape(valid_locs, [1, 1, length, length])\n",
    "        decoder_bias = neg_inf * (1.0 - valid_locs)\n",
    "    return decoder_bias\n",
    "\n",
    "class DecoderStack(tf.keras.layers.Layer):\n",
    "    def __init__(self, params):\n",
    "        super(DecoderStack, self).__init__()\n",
    "        self.params = params\n",
    "        self.layers = []\n",
    "        \n",
    "    def build(self, input_shape):\n",
    "        params = self.params\n",
    "        for _ in range(params['num_hidden_layers']):\n",
    "            self_attention_layer = SelfAttention(\n",
    "                params['hidden_size'],\n",
    "                params['num_heads'],\n",
    "                params['attention_dropout'])\n",
    "            enc_dec_attention_layer = Attention(\n",
    "                params['hidden_size'],\n",
    "                params['num_heads'],\n",
    "                params['attention_dropout'])\n",
    "            feed_forward_network = FeedForwardNetwork(params['hidden_size'], params['filter_size'], params['relu_dropout'])\n",
    "            \n",
    "            self.layers.append([\n",
    "                LayerNormResidualWrapper(self_attention_layer, params),\n",
    "                LayerNormResidualWrapper(enc_dec_attention_layer, params),\n",
    "                LayerNormResidualWrapper(feed_forward_network, params)\n",
    "            ])\n",
    "        self.output_normalization = LayerNormalization(params['hidden_size'])\n",
    "        super(DecoderStack, self).build(input_shape)\n",
    "    \n",
    "    def get_config(self):\n",
    "        return {\n",
    "            'params': self.params\n",
    "        }\n",
    "    \n",
    "    def call(self, decoder_inputs, encoder_outputs, decoder_self_attention_bias, input_attention_bias, training, cache=None):\n",
    "        \"\"\"decoder_inputs: (batch_size, target_length, hidden_size), embedding+position encoding 的输出.\n",
    "           encoder_outputs: (batch_size, input_length, hidden_size), encoder 的输出.\n",
    "           decoder_self_attention_bias: decoder 每个位置后续位置的 mask.\n",
    "           input_attention_bias: 输入的 mask，用于 encoder-decoder attention.\n",
    "        \"\"\"\n",
    "        for n, layer in enumerate(self.layers):\n",
    "            self_attention_layer = layer[0]\n",
    "            enc_dec_attention_layer = layer[1]\n",
    "            feed_forward_network = layer[2]\n",
    "            \n",
    "            layer_name = 'layer_{}'.format(n)\n",
    "            layer_cache = cache[layer_name] if cache is not None else None\n",
    "            with tf.name_scope(layer_name):\n",
    "                with tf.name_scope('self_attention'):\n",
    "                    decoder_inputs = self_attention_layer(\n",
    "                        decoder_inputs,\n",
    "                        decoder_self_attention_bias,\n",
    "                        training=training,\n",
    "                        cache=layer_cache)\n",
    "                with tf.name_scope('encdec_attention'):\n",
    "                    decoder_inputs = enc_dec_attention_layer(\n",
    "                        decoder_inputs,\n",
    "                        encoder_outputs,\n",
    "                        input_attention_bias,\n",
    "                        training=training)\n",
    "                with tf.name_scope('ffn'):\n",
    "                    decoder_inputs = feed_forward_network(decoder_inputs, training=training)\n",
    "        outputs = self.output_normalization(decoder_inputs)\n",
    "        return outputs\n",
    "\n",
    "\n",
    "def test_decoder_stack():\n",
    "    params = {\n",
    "        'num_hidden_layers': 6,\n",
    "        'num_heads': 8,\n",
    "        'hidden_size': 512,\n",
    "        'filter_size': 256,\n",
    "        'attention_dropout': 0.5,\n",
    "        'relu_dropout': 0.5,\n",
    "        'layer_normresidual_dropout': 0.5\n",
    "    }\n",
    "    \n",
    "    layer = DecoderStack(params)\n",
    "    \n",
    "    batch_size = 4\n",
    "    input_length = 16\n",
    "    target_length = 20\n",
    "    decoder_inputs = tf.ones([batch_size, target_length, params['hidden_size']])\n",
    "    encoder_outputs = tf.ones([batch_size, input_length, params['hidden_size']])\n",
    "    decoder_self_attention_bias = get_decoder_self_attetion_bias(target_length)\n",
    "    input_attention_bias = tf.zeros([batch_size, 1, 1, input_length])\n",
    "    outputs = layer(decoder_inputs, encoder_outputs, decoder_self_attention_bias, input_attention_bias, training=True)\n",
    "    print(f'decoder inputs: {decoder_inputs.shape}, encoder outputs: {encoder_outputs.shape} x decoder -> {outputs.shape}')\n",
    "    \n",
    "test_decoder_stack()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Beam Search\n",
    "beam search 细节可以看[Beam Search 原理及实现](https://nbviewer.jupyter.org/github/wzpfish/paper-note/blob/master/notes/nlp/beam_search.ipynb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [],
   "source": [
    "INF = 1. * 1e7\n",
    "\n",
    "class _StateKeys(object):\n",
    "    \"\"\"State 中的 key 的定义\n",
    "    \"\"\"\n",
    "    #TODO: 每个加一个注释.\n",
    "    CUR_INDEX = \"CUR_INDEX\"\n",
    "    ALIVE_SEQ = \"ALIVE_SEQ\"\n",
    "    ALIVE_LOG_PROBS = \"ALIVE_LOG_PROBS\"\n",
    "    ALIVE_CACHE = \"ALIVE_CACHE\"\n",
    "    FINISHED_SEQ = \"FINISHED_SEQ\"\n",
    "    FINISHED_SCORES = \"FINISHED_SCORES\"\n",
    "    FINISHED_FLAGS = \"FINISHED_FLAGS\"\n",
    "\n",
    "\n",
    "class SequenceBeamSearch:\n",
    "    def __init__(self, symbols_to_logits_fn, vocab_size, batch_size, beam_size, alpha, max_decode_length, eos_id):\n",
    "        \"\"\"\n",
    "        \"\"\"\n",
    "        self.symbols_to_logits_fn = symbols_to_logits_fn\n",
    "        self.vocab_size = vocab_size\n",
    "        self.batch_size = batch_size\n",
    "        self.beam_size = beam_size\n",
    "        self.alpha = alpha\n",
    "        self.max_decode_length = max_decode_length\n",
    "        self.eos_id = eos_id\n",
    "        \n",
    "    def search(self, initial_ids, initial_cache):\n",
    "        state, state_shapes = self._create_initial_state(initial_ids, initial_cache)\n",
    "        \n",
    "        finished_state = tf.while_loop(\n",
    "            self._continue_search, self._search_step, loop_vars=[state],\n",
    "            shape_invariants=[state_shapes], parallel_iterations=1, back_prop=False)\n",
    "        finished_state = finished_state[0]\n",
    "\n",
    "        alive_seq = finished_state[_StateKeys.ALIVE_SEQ]\n",
    "        alive_log_probs = finished_state[_StateKeys.ALIVE_LOG_PROBS]\n",
    "        finished_seq = finished_state[_StateKeys.FINISHED_SEQ]\n",
    "        finished_scores = finished_state[_StateKeys.FINISHED_SCORES]\n",
    "        finished_flags = finished_state[_StateKeys.FINISHED_FLAGS]\n",
    "        \n",
    "        # 由于有可能 finished_seq 里一个序列都没有，即没有任何一个序列走到了 eos token，这时候需要把\n",
    "        # alive_seq 作为 backup.\n",
    "        finished_cond = tf.reduce_any(finished_flags, 1, name=\"finished_cond\")\n",
    "        seq_cond = _expand_to_same_rank(finished_cond, finished_seq)\n",
    "        score_cond = _expand_to_same_rank(finished_cond, finished_scores)\n",
    "        finished_seq = tf.where(seq_cond, finished_seq, alive_seq)\n",
    "        finished_scores = tf.where(score_cond, finished_scores, alive_log_probs)\n",
    "        return finished_seq, finished_scores\n",
    "        \n",
    "    def _create_initial_state(self, initial_ids, initial_cache):\n",
    "        \"\"\"inital_ids: 预测时的初始 id (一般设为0)，维度为 (batch_size, )\n",
    "        如果 batch_size 为 3， 则 initial_ids 为 [0, 0, 0]\n",
    "        \"\"\"\n",
    "        # 当前 decode 到哪个位置，初始为 0\n",
    "        cur_index = tf.constant(0)\n",
    "        \n",
    "        # 还没有 decode 完成的 sequence， 即没有decode 到 eos token.\n",
    "        alive_seq = _expand_to_beam_size(initial_ids, self.beam_size)\n",
    "        # (batch_size, beam_size, 1)\n",
    "        alive_seq = tf.expand_dims(alive_seq, axis=2)\n",
    "        \n",
    "        # alive_log_probs 保存每个 batch 每个 beam 下的 sequence 的 log probability。\n",
    "        # 初始化 sequence 的概率为1，即 log prob 为 0.\n",
    "        # 维度为 (batch_size, beam_size)\n",
    "        # 例如，当 batch size 为3， beam size 为4时，alive_log_probs 初始化为:\n",
    "        # [[  0. -inf -inf -inf]\n",
    "        #  [  0. -inf -inf -inf]\n",
    "        #  [  0. -inf -inf -inf]]\n",
    "        initial_log_probs = tf.constant([[0.] + [-float(\"inf\")] * (self.beam_size - 1)])\n",
    "        alive_log_probs = tf.tile(initial_log_probs, [self.batch_size, 1])\n",
    "        \n",
    "        # 将 cache 中保存的每一个变量都加一维 beam_size 维，使得不同 beam 下 cache 的变量不一样。\n",
    "        alive_cache = tf.nest.map_structure(lambda t: _expand_to_beam_size(t, self.beam_size), initial_cache)\n",
    "        \n",
    "        # 初始化用户保存已经预测完成的 sequence 的变量。\n",
    "        finished_seq = tf.zeros(tf.shape(alive_seq), tf.int32)\n",
    "        # 初始化用户保存已经预测完成的 sequence 的 log probability.\n",
    "        finished_scores = tf.ones([self.batch_size, self.beam_size]) * -INF\n",
    "        # 初始化用户保存 sequence 是否已经预测完成的变量。\n",
    "        finished_flags = tf.zeros([self.batch_size, self.beam_size], tf.bool)\n",
    "        \n",
    "        # 初始化 state，这个 state 命名是根据 tf.while_loop 来的（类比 rnn 中的初始化 state）。\n",
    "        state = {\n",
    "            _StateKeys.CUR_INDEX: cur_index,\n",
    "            _StateKeys.ALIVE_SEQ: alive_seq,\n",
    "            _StateKeys.ALIVE_LOG_PROBS: alive_log_probs,\n",
    "            _StateKeys.ALIVE_CACHE: alive_cache,\n",
    "            _StateKeys.FINISHED_SEQ: finished_seq,\n",
    "            _StateKeys.FINISHED_SCORES: finished_scores,\n",
    "            _StateKeys.FINISHED_FLAGS: finished_flags\n",
    "        }\n",
    "        \n",
    "        # 在 tf.while_loop 为了保证正确性，每个 loop 都会检查 state 中变量的 shape 是不是和 shape_invariants 设置的 shape 保持一致。\n",
    "        # 如果不一致，就会报错。因此，如果 state 中的变量在 loop 的时候 shape 会变，则需要把它设置的 general 一点，比如 None。\n",
    "        # 另外，如果 dimension 的值会根据 state 的输入不同而不同，不能提前确定，也要设置成 None，比如 batch size.\n",
    "        state_shape_invariants = {\n",
    "            _StateKeys.CUR_INDEX: tf.TensorShape([]),\n",
    "            _StateKeys.ALIVE_SEQ: tf.TensorShape([None, self.beam_size, None]),\n",
    "            _StateKeys.ALIVE_LOG_PROBS: tf.TensorShape([None, self.beam_size]),\n",
    "            _StateKeys.ALIVE_CACHE: tf.nest.map_structure(\n",
    "                _get_shape_keep_last_dim, alive_cache),\n",
    "            _StateKeys.FINISHED_SEQ: tf.TensorShape([None, self.beam_size, None]),\n",
    "            _StateKeys.FINISHED_SCORES: tf.TensorShape([None, self.beam_size]),\n",
    "            _StateKeys.FINISHED_FLAGS: tf.TensorShape([None, self.beam_size])\n",
    "        }\n",
    "        \n",
    "        return state, state_shape_invariants\n",
    "    \n",
    "    def _continue_search(self, state):\n",
    "        \"\"\"判断 decode 是否应该停止，decode 停止条件有两个：\n",
    "            1. 达到最大 decode 长度。\n",
    "            2. 已经生成的序列的最低分比当前序列的最高分还高，即找不到更好预测序列了。\n",
    "        \"\"\"\n",
    "        i = state[_StateKeys.CUR_INDEX]\n",
    "        alive_log_probs = state[_StateKeys.ALIVE_LOG_PROBS]\n",
    "        finished_scores = state[_StateKeys.FINISHED_SCORES]\n",
    "        finished_flags = state[_StateKeys.FINISHED_FLAGS]\n",
    "    \n",
    "        not_at_max_decode_length = tf.less(i, self.max_decode_length)\n",
    "        max_length_norm = _length_normalization(self.alpha, self.max_decode_length)\n",
    "        \n",
    "        # 为什么是取第0个beam，因为存的时候就是排序好的。\n",
    "        best_alive_scores = alive_log_probs[:, 0] / max_length_norm\n",
    "        \n",
    "        finished_scores *= tf.cast(finished_flags, tf.float32)\n",
    "        # 当前预测完成的序列的最低分, 维度 (batch_size, )\n",
    "        lowest_finished_scores = tf.reduce_min(finished_scores, axis=1)\n",
    "        \n",
    "        # 如果某个batch一个已完成的序列都没有，则把分数设为一个最小值。\n",
    "        finished_batches = tf.reduce_any(finished_flags, 1)\n",
    "        lowest_finished_scores += (1.0 - tf.cast(finished_batches, tf.float32)) * -INF\n",
    "        \n",
    "        worst_finished_score_better_than_best_alive_score = tf.reduce_all(\n",
    "            tf.greater(lowest_finished_scores, best_alive_scores)\n",
    "        )\n",
    "\n",
    "        return tf.logical_and(\n",
    "            not_at_max_decode_length,\n",
    "            tf.logical_not(worst_finished_score_better_than_best_alive_score)\n",
    "        )\n",
    "        \n",
    "    def _search_step(self, state):\n",
    "        # Step 1. 对于每一个 batch 的每一个 beam，都去 decode 下一个 token。并保留 beam_size * 2个概率最高的序列。\n",
    "        # 保留 beam_size * 2 的目的是保证至少有 beam_size 个序列是还没 decode 完成的。例如假如每个 beam 都是 eos token 概率\n",
    "        # 最高，多取一个可以保证能取到非 eos 的 token。\n",
    "        new_seq, new_log_probs, new_cache = self._grow_alive_seq(state)\n",
    "\n",
    "        # Step 2. 从 beam_size * 2 个概率最高的序列中，拿出 beam_size 个概率最高的，且还没有 decode 完成的序列。\n",
    "        alive_state = self._get_new_alive_state(new_seq, new_log_probs, new_cache)\n",
    "        \n",
    "        # Step 3. 把新得到的已完成的序列与原来得到的已完成的序列拼在一起，得到新的 beam_size 个 log prob 最高的「已完成」序列。\n",
    "        finished_state = self._get_new_finished_state(state, new_seq, new_log_probs)\n",
    "        \n",
    "        # Step 4. 更新 state.\n",
    "        new_state = {_StateKeys.CUR_INDEX: state[_StateKeys.CUR_INDEX] + 1}\n",
    "        new_state.update(alive_state)\n",
    "        new_state.update(finished_state)\n",
    "        return [new_state]\n",
    "        \n",
    "    def _get_new_finished_state(self, state, new_seq, new_log_probs):\n",
    "        i = state[_StateKeys.CUR_INDEX]\n",
    "        finished_seq = state[_StateKeys.FINISHED_SEQ]\n",
    "        finished_scores = state[_StateKeys.FINISHED_SCORES]\n",
    "        finished_flags = state[_StateKeys.FINISHED_FLAGS]\n",
    "        \n",
    "        length_norm = _length_normalization(self.alpha, i + 1)\n",
    "        new_scores = new_log_probs / length_norm\n",
    "        \n",
    "        new_finished_flags = tf.equal(new_seq[:, :, -1], self.eos_id)\n",
    "        new_scores += (1 - tf.cast(new_finished_flags, tf.float32)) * -INF\n",
    "        \n",
    "        finished_seq = tf.concat([finished_seq, tf.zeros([self.batch_size, self.beam_size, 1], tf.int32)], axis=2)\n",
    "        \n",
    "        finished_seq = tf.concat([finished_seq, new_seq], axis=1)\n",
    "        finished_scores = tf.concat([finished_scores, new_scores], axis=1)\n",
    "        finished_flags = tf.concat([finished_flags, new_finished_flags], axis=1)\n",
    "        \n",
    "        top_finished_seq, top_finished_scores, top_finished_flags = (\n",
    "            _gather_topk_beams([finished_seq, finished_scores, finished_flags],\n",
    "                               finished_scores, self.batch_size, self.beam_size))\n",
    "        \n",
    "        return {\n",
    "            _StateKeys.FINISHED_SEQ: top_finished_seq,\n",
    "            _StateKeys.FINISHED_SCORES: top_finished_scores,\n",
    "            _StateKeys.FINISHED_FLAGS: top_finished_flags\n",
    "        }\n",
    "        \n",
    "    def _grow_alive_seq(self, state):\n",
    "        \"\"\" 对于还没有decode完成的每一个 sequence，继续decode下一个词，并保留 beam_size * 2 个序列概率最大的序列。\n",
    "        Returns:\n",
    "          topk_seq: 概率最大的topk个序列，shape: (batch_size, beam_size*2, i+2)\n",
    "          topk_log_probs: topk个序列对应的log prob，shape: (batch_size, beam_size*2)\n",
    "          new_cache: 序列对应的 attention 中的 k, v 等信息。\n",
    "        \"\"\"\n",
    "        i = state[_StateKeys.CUR_INDEX]\n",
    "        alive_seq = state[_StateKeys.ALIVE_SEQ]\n",
    "        alive_log_probs = state[_StateKeys.ALIVE_LOG_PROBS]\n",
    "        alive_cache = state[_StateKeys.ALIVE_CACHE]\n",
    "        \n",
    "        beams_to_keep = 2 * self.beam_size\n",
    "        \n",
    "        \n",
    "        # 把 batch_size 和 beam_size 合并，以便喂到模型中。因为模型并不接受 beam_size 这一维\n",
    "        flat_ids = _flatten_beam_dim(alive_seq)\n",
    "        flat_cache = tf.nest.map_structure(_flatten_beam_dim, alive_cache)\n",
    "        flat_logits, flat_cache = self.symbols_to_logits_fn(flat_ids, i, flat_cache)\n",
    "        \n",
    "        # shape: [batch_size, beam_size, vocab_size]\n",
    "        logits = _unflatten_beam_dim(flat_logits, self.batch_size, self.beam_size)\n",
    "        new_cache = tf.nest.map_structure(lambda t: _unflatten_beam_dim(t, self.batch_size, self.beam_size), flat_cache)\n",
    "        \n",
    "        # shape: [batch_size, beam_size, vocab_size] 即下一个词为词表中每个词的 log prob.\n",
    "        candidate_log_probs = _log_prob_from_logits(logits)\n",
    "        log_probs = candidate_log_probs + tf.expand_dims(alive_log_probs, axis=2)\n",
    "        \n",
    "        # 对于每个 batch，都有 beam_size * vocab_size 个 candidate 序列，我们需要从这些序列中找出 log prob 最高的 topk 个。\n",
    "        flat_log_probs = tf.reshape(log_probs, [-1, self.beam_size * self.vocab_size])\n",
    "        topk_log_probs, topk_indices = tf.nn.top_k(flat_log_probs, k=beams_to_keep)\n",
    "        \n",
    "        # shape: (batch_size, beams_to_keep)\n",
    "        topk_beam_indices = topk_indices // self.vocab_size\n",
    "        \n",
    "        topk_seq, new_cache = _gather_beams(\n",
    "            [alive_seq, new_cache], topk_beam_indices, self.batch_size,\n",
    "            beams_to_keep)\n",
    "        \n",
    "        topk_word_ids = topk_indices % self.vocab_size\n",
    "        # shape: (batch_size, beams_to_keep, 1)\n",
    "        topk_word_ids = tf.expand_dims(topk_word_ids, axis=2)\n",
    "        topk_seq = tf.concat([topk_seq, topk_word_ids], axis=2)\n",
    "        return topk_seq, topk_log_probs, new_cache\n",
    "    \n",
    "    def _get_new_alive_state(self, new_seq, new_log_probs, new_cache):\n",
    "        new_finished_flags = tf.equal(new_seq[:, :, -1], self.eos_id)\n",
    "        new_log_probs += tf.cast(new_finished_flags, tf.float32) * -INF\n",
    "        \n",
    "        top_alive_seq, top_alive_log_probs, top_alive_cache = _gather_topk_beams(\n",
    "            [new_seq, new_log_probs, new_cache], new_log_probs, self.batch_size, self.beam_size)\n",
    "        \n",
    "        return {\n",
    "            _StateKeys.ALIVE_SEQ: top_alive_seq,\n",
    "            _StateKeys.ALIVE_LOG_PROBS: top_alive_log_probs,\n",
    "            _StateKeys.ALIVE_CACHE: top_alive_cache\n",
    "        }\n",
    "        \n",
    "def _gather_topk_beams(nested, score_or_log_prob, batch_size, beam_size):\n",
    "    _, topk_indexes = tf.nn.top_k(score_or_log_prob, k=beam_size)\n",
    "    return _gather_beams(nested, topk_indexes, batch_size, beam_size)\n",
    "\n",
    "def _expand_to_beam_size(tensor, beam_size):\n",
    "    \"\"\"给 tensor 添加一维 beam_size 的维度，添加到第一维。比如 tensor 是 (batch_size, ) 则输出是 (batch_size, beam_size) \n",
    "    例如: tensor = [1, 2, 3, 4], beam_size 是3，则结果为: \n",
    "    [\n",
    "     [1, 1, 1]\n",
    "     [2, 2, 2]\n",
    "     [3, 3, 3]\n",
    "     [4, 4, 4]\n",
    "    ]\n",
    "    \"\"\"\n",
    "    tensor = tf.expand_dims(tensor, axis=1)\n",
    "    tile_dims = [1] * tensor.shape.ndims\n",
    "    tile_dims[1] = beam_size\n",
    "    \n",
    "    return tf.tile(tensor, tile_dims)\n",
    "\n",
    "def _get_shape_keep_last_dim(tensor):\n",
    "    \"\"\"只保留 shape 的最后一维，其它都设为 None。\n",
    "    \"\"\"\n",
    "    shape_list = _shape_list(tensor)\n",
    "    \n",
    "    for i in range(len(shape_list) - 1):\n",
    "        shape_list[i] = None\n",
    "    \n",
    "    # 这句话用在什么情况？\n",
    "    if isinstance(shape_list[-1], tf.Tensor):\n",
    "        shape_list[-1] = None\n",
    "    \n",
    "    return tf.TensorShape(shape_list)\n",
    "    \n",
    "def _shape_list(tensor):\n",
    "    shape = tensor.get_shape().as_list()\n",
    "    dynamic_shape = tf.shape(tensor)\n",
    "    for i in range(len(shape)):\n",
    "        if shape[i] is None:\n",
    "            shape[i] = dynamic_shape[i]\n",
    "    return shape\n",
    "    \n",
    "def _length_normalization(alpha, length):\n",
    "    \"\"\"长度归一化，使得 beam search 给短的 sequence 一些惩罚。\n",
    "    \"\"\"\n",
    "    return tf.pow(((5. + tf.cast(length, tf.float32)) / 6.), alpha)\n",
    "\n",
    "def _flatten_beam_dim(tensor):\n",
    "    \"\"\" 合并 batch_size 和 beam_size 这俩维到 batch_size * beam_size 一维。\n",
    "    即 (batch_size, beam_size, ...) -> (batch_size * beam_size, ...)\n",
    "    \"\"\"\n",
    "    shape = _shape_list(tensor)\n",
    "    shape[0] *= shape[1]\n",
    "    shape.pop(1)\n",
    "    return tf.reshape(tensor, shape)\n",
    "\n",
    "def _unflatten_beam_dim(tensor, batch_size, beam_size):\n",
    "    \"\"\" 与 flatten_beam_dim 效果相反。\n",
    "    即：(batch_size * beam_size, ...) -> (batch_size, beam_size, ...)\n",
    "    \"\"\"\n",
    "    shape = _shape_list(tensor)\n",
    "    new_shape = [batch_size, beam_size] + shape[1:]\n",
    "    return tf.reshape(tensor, new_shape)\n",
    "\n",
    "def _log_prob_from_logits(logits):\n",
    "    \"\"\" 计算log概率： log(exp(xi) / sigma(exp(xj)))\n",
    "    \"\"\"\n",
    "    return logits - tf.reduce_logsumexp(logits, axis=2, keepdims=True)\n",
    "\n",
    "def _gather_beams(nested, beam_indices, batch_size, new_beam_size):\n",
    "    # 生成一个 batch_size * new_beam_size 的 tensor，每个 batch 下面都是对应的 batch 下标。\n",
    "    # 例如 batch_size = 2, new_beam_size = 3, 则 batch_pos 为:\n",
    "    # [[0, 0, 0],\n",
    "    #  [1, ,1 ,1]]\n",
    "    batch_pos = tf.range(batch_size * new_beam_size) // new_beam_size\n",
    "    batch_pos = tf.reshape(batch_pos, [batch_size, new_beam_size])\n",
    "    \n",
    "    # 把 batch_pos 和 beam_indices 拼在一起，得到一个 (batch_size, new_beam_size, 2) 的指示下标。\n",
    "    # 最后一维的每个元素都是一个 (batch下标, beam下标).\n",
    "    # 这个是用于传给 tf.gather_nd 来获取对应下标的元素的。\n",
    "    indices = tf.stack([batch_pos, beam_indices], axis=2)\n",
    "    \n",
    "    return tf.nest.map_structure(lambda state: tf.gather_nd(state, indices), nested)\n",
    "\n",
    "def _expand_to_same_rank(tensor, target):\n",
    "  if tensor.shape.rank is None:\n",
    "    raise ValueError(\"Expect rank for tensor shape, but got None.\")\n",
    "  if target.shape.rank is None:\n",
    "    raise ValueError(\"Expect rank for target shape, but got None.\")\n",
    "\n",
    "  with tf.name_scope(\"expand_rank\"):\n",
    "    diff_rank = target.shape.rank - tensor.shape.rank\n",
    "    for _ in range(diff_rank):\n",
    "      tensor = tf.expand_dims(tensor, -1)\n",
    "    return tensor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 完整的 Transformer Model\n",
    "有了各个 layer 的定义，我们就可以定义完整的 transformer 模型了。\n",
    "\n",
    "回顾一下一个 transformer 的组成部分：\n",
    "\n",
    "1. Encoder: inputs -> embedding + position encoding, (multi-head self-attention + feed forward) x N\n",
    "2. Decoder: targets -> embedding + position encoding, (masked multi-head self-attention + encoder-decoder attention + feed forward) x N\n",
    "3. Probabilities: Linear + Softmax\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predict: (2, 3) x transformer -> seq: (2, 6), score: (2,)\n",
      "Train:  inputs-(2, 3) targets-(2, 4) -> logits-(2, 4, 10)\n"
     ]
    }
   ],
   "source": [
    "def get_padding_bias(inputs):\n",
    "    \"\"\"inputs: (batch_size, input_length)\n",
    "    \"\"\"\n",
    "    neg_inf = -1e9\n",
    "    with tf.name_scope('attention_bias'):\n",
    "        padding = tf.cast(tf.equal(inputs, 0), tf.float32)\n",
    "        attention_bias = padding * neg_inf\n",
    "        # 为了适配 attetion layer，reshape 成 (batch_size, 1, 1, length)\n",
    "        attention_bias = tf.expand_dims(tf.expand_dims(attention_bias, axis=1), axis=1)\n",
    "    return attention_bias\n",
    "\n",
    "EOS_ID = 1\n",
    "\n",
    "class Transformer(tf.keras.Model):\n",
    "    def __init__(self, params, name=None):\n",
    "        super(Transformer, self).__init__(name=name)\n",
    "        self.params = params\n",
    "        self.embedding_softmax_layer = EmbeddingSharedWeights(params['vocab_size'], params['hidden_size'])\n",
    "        self.encoder_stack = EncoderStack(params)\n",
    "        self.decoder_stack = DecoderStack(params)\n",
    "    \n",
    "    def get_config(self):\n",
    "        return {\n",
    "            'params': params\n",
    "        }\n",
    "    \n",
    "    def call(self, inputs, training):\n",
    "        \"\"\"inputs[0](x): (batch_size, input_length)\n",
    "           inputs[1](y): (batch_size, target_length)\n",
    "        \"\"\"\n",
    "        if len(inputs) == 2:\n",
    "            inputs, targets = inputs[0], inputs[1]\n",
    "        else:\n",
    "            inputs, targets = inputs[0], None\n",
    "        \n",
    "        with tf.name_scope('Transformer'):\n",
    "            attention_bias = get_padding_bias(inputs)\n",
    "            encoder_outputs = self._encode(inputs, attention_bias, training)\n",
    "            \n",
    "            if targets is None:\n",
    "                return self._predict(encoder_outputs, attention_bias, training)\n",
    "            else:\n",
    "                logits = self._decode(targets, encoder_outputs, attention_bias, training)\n",
    "                return logits\n",
    "    \n",
    "    def _encode(self, inputs, attention_bias, training):\n",
    "        \"\"\"inputs(x): (batch_size, input_length)\n",
    "           attention_bias: (batch_size, 1, 1, input_length)\n",
    "        \"\"\"\n",
    "        with tf.name_scope('encode'):\n",
    "            embedded_inputs = self.embedding_softmax_layer(inputs)\n",
    "            with tf.name_scope('add_pos_encoding'):\n",
    "                length = tf.shape(inputs)[1]\n",
    "                # 这里 pos_encoding 需要 mask 嘛，其实不 mask 也没关系。因为 self-attention 用了 mask，pad 的\n",
    "                # embedding 也不会被用上。\n",
    "                pos_encoding = get_position_encoding(length, self.params['hidden_size'])\n",
    "                encoder_inputs = embedded_inputs + pos_encoding\n",
    "            \n",
    "            if training:\n",
    "                encoder_inputs = tf.nn.dropout(encoder_inputs, rate=self.params['embedding_dropout'])\n",
    "            \n",
    "            return self.encoder_stack(encoder_inputs, attention_bias, training=training)\n",
    "        \n",
    "    def _decode(self, targets, encoder_outputs, attention_bias, training):\n",
    "        \"\"\"targets: (batch_size, target_length)\n",
    "           encoder_outputs: (batch_size, input_length, hidden_size) encoder 的输出。\n",
    "        \"\"\"\n",
    "        with tf.name_scope('decode'):\n",
    "            embedded_targets = self.embedding_softmax_layer(targets)\n",
    "            with tf.name_scope('shift_targets'):\n",
    "                # embedded_targets: (batch_size, target_length, hidden_size)\n",
    "                # 把每个 sequence 向右移一位，即第0维不动，第1维前面pad一个0，并去掉最后一维，第2维不动。\n",
    "                embedded_targets = tf.pad(embedded_targets, [[0, 0], [1, 0], [0, 0]])[:, :-1, :]\n",
    "            with tf.name_scope('add_pos_encoding'):\n",
    "                length = tf.shape(targets)[1]\n",
    "                pos_encoding = get_position_encoding(length, self.params['hidden_size'])\n",
    "                decoder_inputs = embedded_targets + pos_encoding\n",
    "            if training:\n",
    "                decoder_inputs = tf.nn.dropout(decoder_inputs, rate=self.params['embedding_dropout'])\n",
    "            \n",
    "            decoder_self_attention_bias = get_decoder_self_attetion_bias(length)\n",
    "            outputs = self.decoder_stack(\n",
    "                decoder_inputs,\n",
    "                encoder_outputs,\n",
    "                decoder_self_attention_bias,\n",
    "                attention_bias,\n",
    "                training=training)\n",
    "            \n",
    "            logits = self.embedding_softmax_layer(outputs, mode='linear')\n",
    "            return logits\n",
    "    \n",
    "    def _get_symbols_to_logits_fn(self, max_decode_length, training):\n",
    "        pos_encoding = get_position_encoding(max_decode_length + 1, self.params['hidden_size'])\n",
    "        decoder_self_attention_bias = get_decoder_self_attetion_bias(max_decode_length)\n",
    "        \n",
    "        def symbols_to_logits_fn(ids, i, cache):\n",
    "            decoder_input = ids[:, -1:]\n",
    "            decoder_input = self.embedding_softmax_layer(decoder_input)\n",
    "            decoder_input += pos_encoding[i: i+1]\n",
    "            \n",
    "            self_attention_bias = decoder_self_attention_bias[:, :, i:i+1, :i+1]\n",
    "            decoder_outputs = self.decoder_stack(\n",
    "                decoder_input,\n",
    "                cache.get('encoder_outputs'),\n",
    "                self_attention_bias,\n",
    "                cache.get('encoder_decoder_attention_bias'),\n",
    "                training=training,\n",
    "                cache=cache)\n",
    "            \n",
    "            logits = self.embedding_softmax_layer(decoder_outputs, mode='linear')\n",
    "            logits = tf.squeeze(logits, axis=[1])\n",
    "            return logits, cache\n",
    "        \n",
    "        return symbols_to_logits_fn\n",
    "            \n",
    "        \n",
    "    def _predict(self, encoder_outputs, attention_bias, training):\n",
    "        batch_size = tf.shape(encoder_outputs)[0]\n",
    "        input_length = tf.shape(encoder_outputs)[1]\n",
    "        max_decode_length = input_length + self.params[\"extra_decode_length\"]\n",
    "        \n",
    "        symbols_to_logits_fn = self._get_symbols_to_logits_fn(max_decode_length, training)\n",
    "        \n",
    "        initial_ids = tf.zeros([batch_size], dtype=tf.int32)\n",
    "        cache = {\n",
    "            'layer_{}'.format(layer): {\n",
    "                'k': tf.zeros([batch_size, 0, self.params['hidden_size']]),\n",
    "                'v': tf.zeros([batch_size, 0, self.params['hidden_size']])\n",
    "            } for layer in range(self.params['num_hidden_layers'])\n",
    "        }\n",
    "        cache['encoder_outputs'] = encoder_outputs\n",
    "        cache['encoder_decoder_attention_bias'] = attention_bias\n",
    "        \n",
    "        decoded_ids, scores = self._beam_search(\n",
    "            symbols_to_logits_fn, initial_ids, cache, self.params['vocab_size'],\n",
    "            self.params['beam_size'], self.params['alpha'], max_decode_length, EOS_ID)\n",
    "        \n",
    "        top_decode_ids = decoded_ids[:, 0, 1:]\n",
    "        top_scores = scores[:, 0]\n",
    "        \n",
    "        return {\n",
    "            'outputs': top_decode_ids,\n",
    "            'scores': top_scores\n",
    "        }\n",
    "        \n",
    "    def _beam_search(self, symbols_to_logits_fn, initial_ids, initial_cache, \n",
    "                     vocab_size, beam_size, alpha, max_decode_length, eos_id):\n",
    "        batch_size = tf.shape(initial_ids)[0]\n",
    "        sbs = SequenceBeamSearch(symbols_to_logits_fn, vocab_size, batch_size, beam_size, alpha, max_decode_length, eos_id)\n",
    "        return sbs.search(initial_ids, initial_cache)\n",
    "\n",
    "def test_transformer():\n",
    "    params = {\n",
    "        'vocab_size': 10,\n",
    "        'hidden_size': 4,\n",
    "        'num_hidden_layers': 6,\n",
    "        'num_heads': 2,\n",
    "        'embedding_dropout': 0.5,\n",
    "        'attention_dropout': 0.5,\n",
    "        'layer_normresidual_dropout': 0.5,\n",
    "        'relu_dropout': 0.5,\n",
    "        'filter_size': 8,\n",
    "        'extra_decode_length': 3,\n",
    "        'beam_size': 3,\n",
    "        'alpha': 0.6\n",
    "    }\n",
    "    model = Transformer(params)\n",
    "    \n",
    "    inputs = np.array([[1, 0, 3], [1, 2, 0]])\n",
    "    targets = np.array([[1, 2, 3, 4], [2, 3, 4, 0]])\n",
    "    outputs = model([inputs], training=False)\n",
    "    print(f'Predict: {inputs.shape} x transformer -> seq: {outputs[\"outputs\"].shape}, score: {outputs[\"scores\"].shape}')\n",
    "    \n",
    "    outputs = model([inputs, targets], training=True)\n",
    "    print(f'Train:  inputs-{inputs.shape} targets-{targets.shape} -> logits-{outputs.shape}')\n",
    "test_transformer()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loss Layer\n",
    "\n",
    "Loss 采用的是 kl 散度，并且用了 label smoothing。\n",
    "\n",
    "label smoothing 意思是，本来预测一个单词，如果单词在target里，则认为这个词的概率是1，vocab 中的其它词概率为0。加入 label smoothing 后，使得target里的测概率是1-smooth，其他词的概率为 smooth/(vocab_size-1)。\n",
    "\n",
    "另外由于 target 中有 padding，在计算时需要把 padding 的影响去掉。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _pad_tensors_to_same_length(x, y):\n",
    "    \"\"\"让x和y第一维大小相同，用0来填充。\n",
    "    TODO: 有必要吗，logits 和 labels 第一维不是应该一样长的吗？ \n",
    "    \"\"\"\n",
    "    with tf.name_scope('pad_to_same_length'):\n",
    "        x_length = tf.shape(x)[1]\n",
    "        y_length = tf.shape(y)[1]\n",
    "        max_length = tf.maximum(x_length, y_length)\n",
    "        \n",
    "        x = tf.pad(x, [[0, 0], [0, max_length - x_length], [0, 0]])\n",
    "        y = tf.pad(y, [[0, 0], [0, max_length - y_length]])\n",
    "        return x, y\n",
    "    \n",
    "def padded_cross_entropy_loss(logits, labels, smoothing, vocab_size):\n",
    "    \"\"\"logits: (batch_size, logits_length, vocab_size)\n",
    "       labels: (batch_size, target_length)\n",
    "    \"\"\"\n",
    "    with tf.name_scope('loss'):\n",
    "        logits, labels = _pad_tensors_to_same_length(logits, labels)\n",
    "        \n",
    "        # label词概率为 1-smoothing。非label词概率为 (smoothing)/(vocab_size-1)\n",
    "        with tf.name_scope('smoothing_cross_entropy'):\n",
    "            confidence = 1.0 - smoothing\n",
    "            low_confidence = smoothing / tf.cast(vocab_size - 1, tf.float32)\n",
    "            soft_targets = tf.one_hot(tf.cast(labels, tf.int32), depth=vocab_size, on_value=confidence, off_value=low_confidence)\n",
    "            xentropy = tf.nn.softmax_cross_entropy_with_logits(logits=logits, labels=soft_targets)\n",
    "           \n",
    "        # 相当于计算 KL-散度作为loss\n",
    "        normalizing_constant = -(\n",
    "            confidence * tf.math.log(confidence) +\n",
    "            tf.cast(vocab_size - 1, tf.float32) * low_confidence *\n",
    "            tf.math.log(low_confidence + 1e-20))\n",
    "        xentropy -= normalizing_constant\n",
    "        \n",
    "        weights = tf.cast(tf.not_equal(labels, 0), tf.float32)\n",
    "        return xentropy * weights, weights\n",
    "\n",
    "def transformer_loss(logtis, labels, smoothing, vocab_size):\n",
    "    xentropy, weights = padded_cross_entropy_loss(logits, labels, smoothing, vocab_size)\n",
    "    return tf.reduce_sum(xentropy) / tf.reduce_sum(weights)\n",
    "\n",
    "class LossLayer(tf.keras.layers.Layer):\n",
    "    def __init__(self, vocab_size, label_smoothing):\n",
    "        super(LossLayer, self).__init__()\n",
    "        self.vocab_size = vocab_size\n",
    "        self.label_smoothing = label_smoothing\n",
    "        \n",
    "    def get_config(self):\n",
    "        return {\n",
    "            'vocab_size': vocab_size,\n",
    "            'label_smoothing': label_smoothing\n",
    "        }\n",
    "    \n",
    "    def call(self, inputs):\n",
    "        logits, targets = inputs[0], inputs[1]\n",
    "        loss = transformer_loss(logits, targets, self.label_smoothing, self.vocab_size)\n",
    "        self.add_loss(loss)\n",
    "        return logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_model(params, is_train):\n",
    "    with tf.name_scope('model'):\n",
    "        if is_train:\n",
    "            inputs = tf.keras.layers.Input((None,), dtype='int64', name='inputs')\n",
    "            targets = tf.keras.layers.Input((None,), dtype='int64', name='targets')\n",
    "            internal_model = Transformer(params, name=\"transformer_v2\")\n",
    "            logits = internal_model([inputs, targets], training=is_train)\n",
    "            vocab_size = params['vocab_size']\n",
    "            label_smoothing = params[\"label_smoothing\"]\n",
    "            logits = metrics.LossLayer(vocab_size, label_smoothing)([logits, targets])\n",
    "            logits = tf.keras.layers.Lambda(lambda x: x, name=\"logits\")(logits)\n",
    "            return tf.keras.Model([inputs, targets], logits)\n",
    "        else:\n",
    "            inputs = tf.keras.layers.Input((None,), dtype=\"int64\", name=\"inputs\")\n",
    "            internal_model = Transformer(params, name=\"transformer_v2\")\n",
    "            ret = internal_model([inputs], training=is_train)\n",
    "            outputs, scores = ret[\"outputs\"], ret[\"scores\"]\n",
    "            return tf.keras.Model(inputs, [outputs, scores])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 总结\n",
    "\n",
    "Transformer 模型在论文中看起来比较简单，但是真正实现起来还是有许多细节要注意，主要有：\n",
    "\n",
    "* embedding与输出全连接层共享权重\n",
    "* position encoding 的含义\n",
    "* 三个原理相同但应用地方不同的 attention\n",
    "* layer normalization用在哪一层\n",
    "* beam search的实现\n",
    "* loss 的计算\n",
    "\n",
    "总之，花点时间读+抄了一遍 tranformer 的代码，对照原始论文，还是有新收获的。\n",
    "\n",
    "## Reference\n",
    "* [tensorflow official transformer implementation](https://github.com/tensorflow/models/tree/master/official/transformer)\n",
    "* [Attention is all you need](https://arxiv.org/abs/1706.03762)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:py36]",
   "language": "python",
   "name": "conda-env-py36-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
